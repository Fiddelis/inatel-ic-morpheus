{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o","text":"<p> Site criado para documentar os estudos referente a Inicia\u00e7\u00e3o Cientifica no CS&amp;I Lab.</p> <p>Contatos</p> <p> Email: lucas.ruan@ges.inatel.br</p> <p> Linkedin: lucas-ruan-fidelis</p> <p> Github: Fiddelis@GitHub</p>"},{"location":"#assuntos-estudados","title":"Assuntos Estudados","text":""},{"location":"#morpheus","title":"Morpheus","text":"<p>O Morpheus \u00e9 uma Framework de seguran\u00e7a cibern\u00e9tica da NVIDIA de c\u00f3digo aberto, acelerada por GPU/DPU, que permite a cria\u00e7\u00e3o de pipelines otimizados para filtrar, processar e classificar um grande volume de dados na rede em tempo real.</p> <p>Utilizando Machine Learning para identificar, capturar e agir sobre as amea\u00e7as.</p>"},{"location":"#beneficios","title":"Benef\u00edcios","text":"<ul> <li>Permite que desenvolvedores criem suas pr\u00f3prias solu\u00e7\u00f5es de seguran\u00e7a cibern\u00e9tica.</li> <li>Permite a adapta\u00e7\u00e3o din\u00e2mica a feedback humano e eventos externos, melhorando a precis\u00e3o e a capacidade de resposta dos modelos de IA.</li> <li>Poss\u00edvel Implementar o pr\u00f3prio modelo ou alguns dos modelos pr\u00e9-treinados e testados pela pr\u00f3pria NVIDIA.</li> </ul>"},{"location":"#triton-inference-server","title":"Triton Infer\u00eance Server","text":"<p>O Triton Inference Server fornece uma solu\u00e7\u00e3o de infer\u00eancia de nuvem e borda otimizada por CPUs e GPUs. O Triton suporta um protocolo HTTP/REST e GRPC que permite que clientes remotos solicitem infer\u00eancia para qualquer modelo gerenciado pelo servidor.</p>"},{"location":"Morpheus/container_morpheus/","title":"Morpheus atr\u00e1vez do Docker","text":"<p>A NVIDIA disponibiliza Imagens pr\u00e9-configuradas de containers que nos permite utilizar o Morpheus sem a necessidade de ter instalado na sua pr\u00f3pria maquina, permitindo a execu\u00e7\u00e3o de testes de forma r\u00e1pida e funcional.</p> <p>No momento que estou escrevendo esse documento, o Morpheus se encontra na vers\u00e3o 24.06(LTE).</p>"},{"location":"Morpheus/container_morpheus/#pre-requisitos","title":"Pr\u00e9-requisitos:","text":"<ul> <li>Arquitetura Volta de GPU ou melhor.</li> <li>CUDA 12.1</li> <li>Docker</li> <li>The NVIDIA Container Toolkit</li> </ul>"},{"location":"Morpheus/container_morpheus/#download-da-imagem","title":"Download da Imagem","text":"<p>Com o Docker ja instalado na sua maquina, precisamos baixar a imagem na nuvem da NVIDIA (NGC).</p> <pre><code>docker pull nvcr.io/nvidia/morpheus/morpheus:24.06-runtime\n</code></pre> <p>Para verificar se o comando funcionou corretamente, execute o comando abaixo para listar todas as imagens instaladas no Docker:</p> <pre><code>docker ps\n</code></pre> Failure <p>Caso a imagem n\u00e3o se encontre na lista, verifique se inseriu a tag corretamente e tente novamente.</p>"},{"location":"Morpheus/container_morpheus/#executando-o-morpheus","title":"Executando o Morpheus","text":"<p>Para Utilizar o Morpheus basta executar o seguinte comando no terminal:</p> <pre><code>docker run --rm -ti --runtime=nvidia --gpus=all --network=host nvcr.io/nvidia/morpheus/morpheus:24.06-runtime bash\n</code></pre> <p>Isso vai fazer com que um container seja criado e executado no seu terminal, ao finalizar a sess\u00e3o ser\u00e1 apagado o container junto com os arquivos que foram gerados.</p> <p>Dentro do Bash do container \u00e9 possivel executar qualquer tipo de comando relacionado ao Morpheus, tal qual:</p> <pre><code>morpheus run --help\n</code></pre>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/","title":"Pipelines com CLI","text":"<p>As cria\u00e7\u00f5es de pipelines do Morpheus podem ser feitas atr\u00e1ves do CLI, onde os est\u00e1gios s\u00e3o executados de forma linear, significando que a sa\u00edda de um est\u00e1gio \u00e9 a entrada do pr\u00f3ximo.</p> <p>Note</p> <p>Existem diferentes tipos de comandos possiveis no Morpheus, para saber mais sobre cada uma delas, verifique utilizando:</p> <pre><code>morpheus run --help\n</code></pre> <p>Ou na pr\u00f3pria documenta\u00e7\u00e3o do Morpheus.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#exemplo","title":"Exemplo","text":"<p>Para explicar o funcionamento a partir do CLI, ser\u00e1 utilizado um exemplo de execu\u00e7\u00e3o de uma Pipeline para detec\u00e7\u00e3o de minera\u00e7\u00e3o de criptomoedas na GPU.</p> <p>Warning</p> <p>Para a execu\u00e7\u00e3o desse exemplo, \u00e9 necess\u00e1rio que voc\u00ea tenha clonado o reposit\u00f3rio do Morpheus;</p> <pre><code>MORPHEUS_ROOT=$(pwd)/morpheus\ngit clone https://github.com/nv-morpheus/Morpheus.git $MORPHEUS_ROOT\ncd $MORPHEUS_ROOT\n</code></pre> <p>e que tenha executado o script para o download dos arquivos maiores.</p> <pre><code>scripts/fetch_data.py fetch examples models\n</code></pre> <p>ou se preferir, verifique a documenta\u00e7\u00e3o do Morpheus na sess\u00e3o Git LFS.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#executando-triton","title":"Executando Triton","text":"<p>Antes que voc\u00ea execute o Pipeline no Morpheus, \u00e9 necess\u00e1rio que voc\u00ea inicialize o modelo com o Triton Server.</p> <pre><code>docker run --rm -ti --gpus=all -p8000:8000 -p8001:8001 -p8002:8002\\\n    -v $PWD/models:/models nvcr.io/nvidia/tritonserver:23.06-py3 tritonserver\\\n        --model-repository=/models/triton-model-repo\\\n        --exit-on-error=false\\\n        --model-control-mode=explicit\\\n        --load-model abp-nvsmi-xgb\n</code></pre> <p>obs.: Esse comando considera que voc\u00ea esteja na pasta root do Morpheus.</p> <p>Ao subir o modelo, \u00e9 preciso que tenha exibido o seguinte resultado:</p> <pre><code>+-------------------+---------+--------+\n| Model             | Version | Status |\n+-------------------+---------+--------+\n| abp-nvsmi-xgb     | 1       | READY  |\n+-------------------+---------+--------+\n</code></pre>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#executando-o-morpheus","title":"Executando o Morpheus","text":"<p>O c\u00f3digo a ser executado nesse exemplo ser\u00e1:</p> <pre><code>morpheus --log_level=DEBUG \\\n    run --num_threads=8 --pipeline_batch_size=1024 --model_max_batch_size=1024 \\\n    pipeline-fil --columns_file=data/columns_fil.txt \\\n    from-file --filename=examples/data/nvsmi.jsonlines \\\n    deserialize \\\n    preprocess \\\n    inf-triton --model_name=abp-nvsmi-xgb --server_url=localhost:8000 \\\n    monitor --description \"Inference Rate\" --smoothing=0.001 --unit inf \\\n    add-class \\\n    serialize --include 'mining' \\\n    to-file --filename=detections.jsonlines --overwrite\n</code></pre> <p>Para o entendimento, iremos analisar linha a linha do comando.</p> <pre><code># Define o nivel do log para o modo de Debug\n\nmorpheus --log_level=DEBUG \\\n</code></pre> <pre><code># Executa um pipeline com 8 threads e um tamanho de lote de modelo de 1024\n# (deve ser igual ou menor que a configura\u00e7\u00e3o do Triton)\n\nrun --num_threads=8 --pipeline_batch_size=1024 --model_max_batch_size=1024 \\\n</code></pre> <pre><code># Especifica um pipeline FIL com 256 de comprimento de sequ\u00eancia\n# (deve corresponder \u00e0 configura\u00e7\u00e3o do Triton)\n\npipeline-fil --columns_file=data/columns_fil.txt \\\n</code></pre> <pre><code># Faz a leitura de um arquivo na pasta especificada\n\nfrom-file --filename=examples/data/nvsmi.jsonlines \\\n</code></pre> <pre><code># Disserializa o arquivo para poder ser processado\n\ndeserialize \\\n</code></pre> <pre><code># Pr\u00e9-processamento que converte as entradas para um token BERT\n\npreprocess \\\n</code></pre> <pre><code># Envia mensagem para o Triton fazer a inferencia\n# (\u00e9 necess\u00e1rio especificar o modelo)\n\ninf-triton --model_name=abp-nvsmi-xgb --server_url=localhost:8000 \\\n</code></pre> <pre><code># Monitora o est\u00e1gio anterior e exibe no console as informa\u00e7\u00f5es de performance\n\nmonitor --description \"Inference Rate\" --smoothing=0.001 --unit inf \\\n</code></pre> <pre><code># Adiciona o resultado de infer\u00eancia na mensagem\n\nadd-class \\\n</code></pre> <pre><code># Converte a mensagem de Objeto para String novamente,\n# contendo somente as informa\u00e7\u00f5es resultantes da infer\u00eancia\n\nserialize --include 'mining' \\\n</code></pre> <pre><code># Faz a escrita do resultado em um arquivo .jsonline\n\nto-file --filename=detections.jsonlines --overwrite\n</code></pre> <p>A ordem de execu\u00e7\u00e3o do Pipeline foi linear, onde a sa\u00edda do n\u00f3 anterior vai para a entrada do pr\u00f3ximo n\u00f3.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#resultado","title":"Resultado","text":"<p>Ao concluir a execu\u00e7\u00e3o do Pipeline, teremos os seguintes resultados.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#console","title":"Console","text":"<pre><code>...\n====Registering Pipeline====\n====Registering Pipeline Complete!====\n====Starting Pipeline====\n====Pipeline Started====\n====Building Pipeline====\nAdded source: &lt;from-file-0; FileSourceStage(filename=examples/data/nvsmi.jsonlines, iterative=False, file_type=FileTypes.Auto, repeat=1, filter_null=True)&gt;\n  \u2514\u2500&gt; morpheus.MessageMeta\nAdded stage: &lt;deserialize-1; DeserializeStage()&gt;\n  \u2514\u2500 morpheus.MessageMeta -&gt; morpheus.MultiMessage\nAdded stage: &lt;preprocess-fil-2; PreprocessFILStage()&gt;\n  \u2514\u2500 morpheus.MultiMessage -&gt; morpheus.MultiInferenceFILMessage\nAdded stage: &lt;inference-3; TritonInferenceStage(model_name=abp-nvsmi-xgb, server_url=localhost:8000, force_convert_inputs=False, use_shared_memory=False)&gt;\n  \u2514\u2500 morpheus.MultiInferenceFILMessage -&gt; morpheus.MultiResponseMessage\nAdded stage: &lt;monitor-4; MonitorStage(description=Inference Rate, smoothing=0.001, unit=inf, delayed_start=False, determine_count_fn=None)&gt;\n  \u2514\u2500 morpheus.MultiResponseMessage -&gt; morpheus.MultiResponseMessage\nAdded stage: &lt;add-class-5; AddClassificationsStage(threshold=0.5, labels=[], prefix=)&gt;\n  \u2514\u2500 morpheus.MultiResponseMessage -&gt; morpheus.MultiResponseMessage\nAdded stage: &lt;serialize-6; SerializeStage(include=['mining'], exclude=['^ID$', '^_ts_'], fixed_columns=True)&gt;\n  \u2514\u2500 morpheus.MultiResponseMessage -&gt; morpheus.MessageMeta\nAdded stage: &lt;to-file-7; WriteToFileStage(filename=detections.jsonlines, overwrite=True, file_type=FileTypes.Auto)&gt;\n  \u2514\u2500 morpheus.MessageMeta -&gt; morpheus.MessageMeta\n====Building Pipeline Complete!====\nStarting! Time: 1656353254.9919598\nInference Rate[Complete]: 1242inf [00:00, 1863.04inf/s]\n====Pipeline Complete====\n</code></pre> <p>Note</p> <p>Como utilizamos a configura\u00e7\u00e3o de Debug no Log, tivemos informa\u00e7\u00f5es de tudo que est\u00e1va acontecendo na cria\u00e7\u00e3o dos est\u00e1gios.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#arquivo-resultante","title":"Arquivo resultante","text":"<pre><code>...\n{\"mining\": 0}\n{\"mining\": 0}\n{\"mining\": 0}\n{\"mining\": 0}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n{\"mining\": 1}\n...\n</code></pre> <p>Note</p> <p>Foi configurado no est\u00e1gio para ser gerado apenas o resultado da classifica\u00e7\u00e3o da infer\u00eancia.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_cli/#executando-estagios-em-python-com-cli","title":"Executando Est\u00e1gios em Python com CLI","text":"<p>Como alternativa ao Pipeline com Python, poder\u00edamos testar o est\u00e1gio baseado em classe em um pipeline constru\u00eddo usando a ferramenta de linha de comando Morpheus. Precisaremos passar o caminho para nosso est\u00e1gio por meio do\u00a0argumento <code>--plugin</code> para que ele fique vis\u00edvel para a ferramenta de linha de comando.</p> <p>Warning</p> <p>At\u00e9 o momento s\u00f3 \u00e9 poss\u00edvel registar o est\u00e1gio feito em classes.</p> <pre><code>morpheus --log_level=debug --plugin examples/developer_guide/1_simple_python_stage/pass_thru.py \\\n  run pipeline-other \\\n  from-file --filename=examples/data/email_with_addresses.jsonlines \\\n  pass-thru \\\n  monitor\n</code></pre>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_python/","title":"Pipelines com Python","text":"<p>Os est\u00e1gios do Morpheus s\u00e3o conectados iguais a um grafo, onde cada n\u00f3 \u00e9 um est\u00e1gio que recebe os dados e retornam outro, \u00e9 preciso deixar claro os tipos dos dados de entrada e de sa\u00edda para que o Morpheus possa conferir na hora da execu\u00e7\u00e3o da pipeline.</p> <p>\u00c9 poss\u00edvel criar uma fun\u00e7\u00e3o ou uma classe para o est\u00e1gio, permitindo mais flexibilidade na cria\u00e7\u00e3o do est\u00e1gio.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_python/#funcoes-como-estagio","title":"Fun\u00e7\u00f5es como Est\u00e1gio","text":"<p>Toda fun\u00e7\u00e3o precisa do decorador <code>@stage</code> que \u00e9 responsavel por deixar claro para o Morpheus que se trata de uma fun\u00e7\u00e3o que ser\u00e1 utilizada como um est\u00e1gio a ser executado.</p> <pre><code>@stage\ndef pass_thru_stage(message: typing.Any) -&gt; typing.Any:\n    # Retorna mensagem para o proximo est\u00e1gio (aqui ficaria a l\u00f3gica do est\u00e1gio)\n    return message\n\n# --- Implementando no Pipeline ---\nconfig = Config()\npipeline = LinearPipeline(config)\n# ...\npipeline.add_stage(pass_thru_stage(config))\n</code></pre> <p>\u00c9 preciso tamb\u00e9m deixar claro o tipo da mensagem que ser\u00e1 recebida <code>message: typing.Any</code> e o tipo do retorno <code>-&gt; typing.Any</code> para que o Morpheus possa conferir ao criar o n\u00f3.</p>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_python/#classes-como-estagio","title":"Classes como Est\u00e1gio","text":"<p>Ao optarmos por utilizar uma classe, precisamos especificar o tipo da classe herdando alguma pr\u00e9-definida pelo Morpheus, alguns exemplos:</p> Tipo Descri\u00e7\u00e3o SinglePortStage Est\u00e1gios que cont\u00e9m apenas uma entrada e sa\u00edda. SingleOutputSource Est\u00e1gios que atuam como fontes de dados, no sentido que n\u00e3o recebe uma entrada de um est\u00e1gio anterior, por exemplo: algum arquivo ou mensageria com Kafka. PassThruTypeMixin Define que o tipo da mensagem de entrada \u00e9 o mesmo do de sa\u00edda (muito comum no Morpheus). <p>Note</p> <p>Caso queira utilizar a classe especificada no CLI, precisamos utilizar um decorador na classe chamado de <code>@register_stage(\"pass-thru\")</code> fazendo com que seja poss\u00edvel a detec\u00e7\u00e3o na hora da cria\u00e7\u00e3o da pipeline. <pre><code>@register_stage(\"pass-thru\")\nclass PassThruStage(PassThruTypeMixin, SinglePortStage):\n</code></pre></p> <p>\u00c9 preciso criar 5 m\u00e9todos na subclasse para implementar a interface stage: <code>name</code> , <code>accepted_types</code> , <code>compute_schema</code> , <code>supports_cpp_node</code> e <code>_build_single</code> . Na pr\u00e1tica \u00e9 necess\u00e1rio definir mais um m\u00e9todo que executar\u00e1 o trabalho real do est\u00e1gio, por conven\u00e7\u00e3o se chama <code>on_data</code> .</p> <p>obs.: \u00e9 necess\u00e1rio colocar a anota\u00e7\u00e3o @property antes de inserir esses m\u00e9todos</p> nameaccepted_typescompute_schemasupports_cpp_node_build_singleon_data <p>Utilizada para retornar um nome mais amig\u00e1vel para o est\u00e1gio, usada apenas para fins de depura\u00e7\u00e3o. <pre><code>def name(self) -&gt; str:\n    return \"pass-thru\"\n</code></pre></p> <p>Retorna uma tupla de classes de mensagem que este est\u00e1gio \u00e9 capaz de aceitar como entrada. (Permite que o Morpheus valide se a sa\u00edda do pai desse est\u00e1gio \u00e9 o mesmo da entrada). <pre><code>def accepted_types(self) -&gt; tuple:\n    return (typing.Any,)\n</code></pre></p> <p>Retorna o tipo de sa\u00edda do est\u00e1gio (Como vamos herdar da classe <code>PassThruTypeMixin</code>, n\u00e3o precisamos utiliza-la): <pre><code>def compute_schema(self, schema: StageSchema):\n    schema.output_schema.set_type(schema.input_type)\n</code></pre></p> <p>Retorna se estamos utilizando arquivos CPP nesse est\u00e1gio. <pre><code>def supports_cpp_node(self) -&gt; bool:\n    return False\n</code></pre></p> <p>M\u00e9todo que ser\u00e1 usado no momento de constru\u00e7\u00e3o do est\u00e1gio, para construir o n\u00f3 e conecta-lo ao pipeline, ele recebe uma inst\u00e2ncia de um construtor do mrc(Morpheus Runtime Core) do tipo <code>mrc.Builder</code>, junto com um n\u00f3 de entrada do tipo <code>SegmentObject</code> e retorna um n\u00f3 rec\u00e9m constru\u00eddo. <pre><code>def _build_single(self, builder: mrc.Builder, input_node: mrc.SegmentObject) -&gt; mrc.SegmentObject:\n    node = builder.make_node(self.unique_name, ops.map(self.on_data))\n    builder.make_edge(input_node, node)\n\n    return node\n</code></pre></p> <p>Linha a linha:</p> <p><code>node = builder.make_node(self.unique_name, ops.map(self.on_data))</code></p> <p>Cria um n\u00f3 com o nome que demos para a classe junto a um ID \u00fanico para que n\u00e3o seja poss\u00edvel existir est\u00e1gios com mesmo nome;</p> <p><code>builder.make_edge(input_node, node)</code></p> <p>Define uma aresta conectando o novo n\u00f3 ao n\u00f3 pai.</p> <p>Aceita uma mensagem recebida e retorna uma mensagem (l\u00f3gica do est\u00e1gio) <pre><code>def on_data(self, message: typing.Any):\n    # Retorna mensagem para o proximo est\u00e1gio\n    return message\n</code></pre></p> <p>A Classe completa ficaria assim:</p> <pre><code>import typing\n\nimport mrc\nfrom mrc.core import operators as ops\n\nfrom morpheus.cli.register_stage import register_stage\nfrom morpheus.pipeline.pass_thru_type_mixin import PassThruTypeMixin\nfrom morpheus.pipeline.single_port_stage import SinglePortStage\n\n\n@register_stage(\"pass-thru\")\nclass PassThruStage(PassThruTypeMixin, SinglePortStage):\n\n    @property\n    def name(self) -&gt; str:\n        return \"pass-thru\"\n\n    def accepted_types(self) -&gt; tuple:\n        return (typing.Any, )\n\n    def supports_cpp_node(self) -&gt; bool:\n        return False\n\n    def on_data(self, message: typing.Any):\n        # Return the message for the next stage\n        return message\n\n    def _build_single(self, builder: mrc.Builder, input_node: mrc.SegmentObject) -&gt; mrc.SegmentObject:\n        node = builder.make_node(self.unique_name, ops.map(self.on_data))\n        builder.make_edge(input_node, node)\n\n        return node\n</code></pre>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_python/#criacao-de-um-pipeline-simples","title":"Cria\u00e7\u00e3o de um Pipeline simples","text":"<p>Antes de construir o pipeline \u00e9 preciso configurar o ambiente, come\u00e7ando com o log:</p> <pre><code>configure_logging(log_level=logging.DEBUG)\n</code></pre> <p>Os manipuladores de registro n\u00e3o s\u00e3o bloqueantes, pois utilizam uma fila para enviar as mensagens de registro em um thread separado.</p> <p>Note</p> <p>Podemos utilizar a fun\u00e7\u00e3o\u00a0<code>configure_logging</code> para adicionar um ou mais manipuladores de registro \u00e0 configura\u00e7\u00e3o padr\u00e3o, um exemplo disso seria:</p> <pre><code>loki_handler = logging_loki.LokiHandler(\n    url=f\"{loki_url}/loki/api/v1/push\",\n    tags={\"app\": \"morpheus\"},\n    version=\"1\",\n)\n\nconfigure_logging(loki_handler, log_level=log_level)\n</code></pre> <p>Onde \u00e9 adicionado um manipulador de log chamado Grafana Loki que cria logs \u00fanicos para depura\u00e7\u00e3o posteriormente.</p> <p>Ap\u00f3s configurarmos o Log, precisamos criar uma inst\u00e2ncia de configura\u00e7\u00e3o (sempre ser\u00e1 necess\u00e1rio):</p> <pre><code>config = Config()\n</code></pre> <p>Para o exemplo ser\u00e1 utilizado a classe <code>FileSourceStage</code> , para ler um arquivo grande no qual cada linha \u00e9 um objeto JSON. O est\u00e1gio pegar\u00e1 essas linhas e as empacotar\u00e1 como objetos de mensagem Morpheus para nosso est\u00e1gio de passagem consumir.</p> <pre><code>pipeline.set_source(FileSourceStage(config, filename=input_file, iterative=False))\n</code></pre> <p>Em Seguida \u00e9 adicionado os est\u00e1gios ao pipeline, junto com a inst\u00e2ncia <code>MonitorStage</code> , para monitorar a taxa de transfer\u00eancia dos est\u00e1gios:</p> <pre><code># Adiciona a fun\u00e7\u00e3o de est\u00e1gio com o decorador @stage\npipeline.add_stage(pass_thru_stage(config))\n\n# Adiciona o monitoramento de performance do est\u00e1gio anterior\npipeline.add_stage(MonitorStage(config))\n\n# Adiciona a classe criada como est\u00e1gio\npipeline.add_stage(PassThruStage(config))\n\n# Adiciona o monitoramento de performance do est\u00e1gio anterior\npipeline.add_stage(MonitorStage(config))\n</code></pre>"},{"location":"Morpheus/Cria%C3%A7%C3%A3o%20de%20Pipelines/pipelines_com_python/#pipeline-completo","title":"Pipeline Completo","text":"<pre><code>import logging\nimport os\n\nfrom pass_thru import PassThruStage\nfrom pass_thru_deco import pass_thru_stage\n\nfrom morpheus.config import Config\nfrom morpheus.pipeline import LinearPipeline\nfrom morpheus.stages.general.monitor_stage import MonitorStage\nfrom morpheus.stages.input.file_source_stage import FileSourceStage\nfrom morpheus.utils.logger import configure_logging\n\ndef run_pipeline():\n    # Configura o Log do Morpheus\n    configure_logging(log_level=logging.DEBUG)\n\n    root_dir = os.environ['MORPHEUS_ROOT']\n    input_file = os.path.join(root_dir, 'examples/data/email_with_addresses.jsonlines')\n\n    config = Config()\n\n    # Cria um Objeto para pipeline linear (a saida de um est\u00e1gio \u00e9 a entrada da outra na ordem que foi criada)\n    pipeline = LinearPipeline(config)\n\n    # Faz a leitura do arquivo JSON\n    pipeline.set_source(FileSourceStage(config, filename=input_file, iterative=False))\n\n    # Adiciona a fun\u00e7\u00e3o de est\u00e1gio com o decorador @stage\n    pipeline.add_stage(pass_thru_stage(config))\n\n    # Adiciona o monitoramento de performance do est\u00e1gio anterior\n    pipeline.add_stage(MonitorStage(config))\n\n    # Adiciona a classe criada como est\u00e1gio\n    pipeline.add_stage(PassThruStage(config))\n\n    # Adiciona o monitoramento de performance do est\u00e1gio anterior\n    pipeline.add_stage(MonitorStage(config))\n\n    pipeline.run()\n\nif __name__ == \"__main__\":\n    run_pipeline()\n</code></pre>"},{"location":"Triton%20Inference%20Server/container_triton/","title":"Triton Server atr\u00e1ves do Docker","text":"<p>Muitos casos de testes onde \u00e9 necess\u00e1rio a analise dos dados a partir de uma IA \u00e9 preciso que tenha em execu\u00e7\u00e3o o Triton Inference Server, \u00e9 ele quem cuida da classifica\u00e7\u00e3o dos dados a partir do seu modelo pr\u00e9-treinado.</p> <p>No momento que estou escrevendo esse documento, o Triton Inference Server se encontra na vers\u00e3o 24.09.</p>"},{"location":"Triton%20Inference%20Server/container_triton/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Docker</li> </ul>"},{"location":"Triton%20Inference%20Server/container_triton/#download-da-imagem","title":"Download da Imagem","text":"<p>Com o Docker ja instalado na sua maquina, precisamos baixar a imagem na nuvem da NVIDIA (NGC).</p> <pre><code>docker pull nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3\n</code></pre> <p>Para verificar se o comando funcionou corretamente, execute o comando abaixo para listar todas as imagens instaladas no Docker:</p> <pre><code>docker ps\n</code></pre> Failure <p>Caso a imagem n\u00e3o se encontre na lista, verifique se inseriu a tag corretamente e tente novamente.</p>"},{"location":"Triton%20Inference%20Server/container_triton/#executando-o-triton-server","title":"Executando o Triton Server","text":"<p>A execu\u00e7\u00e3o do Triton Server depende do tipo do modelo que ser\u00e1 executado, oque varia de projeto para projeto.</p> <p>Exemplo de execu\u00e7\u00e3o:</p> <pre><code>docker run --rm -ti --gpus=all -p8000:8000 -p8001:8001 -p8002:8002\\\n    -v $PWD/models:/models\\\n    nvcr.io/nvidia/tritonserver:23.06-py3 tritonserver\\\n    --model-repository=/models/triton-model-repo\\\n    --exit-on-error=false\\\n    --log-info=true\\\n    --strict-readiness=false\\\n    --disable-auto-complete-config\n</code></pre> <p>Esse comando faz com que seja executado um cantainer docker do Triton que execute todos os modelos na pasta de reposit\u00f3rio especificada.</p> <p>Note</p> <p>Como n\u00e3o foi especificado nenhum modelo, o Triton Server ir\u00e1 executar todos os modelos que encontrar na pasta de reposit\u00f3rio.</p> <p>Para especificar o modelo utilize o comando: <pre><code>--load-model {{NOME-DO-MODELO}}\n</code></pre></p> <p>Depois que o Triton carregar o modelo, ser\u00e1 exibido algo parecido com:</p> <pre><code>+-------------------+---------+--------+\n| Model             | Version | Status |\n+-------------------+---------+--------+\n| {NOME-DO-MODELO}  | 1       | READY  |\n+-------------------+---------+--------+\n</code></pre> Failure <p>Se isso n\u00e3o estiver presente na sa\u00edda, verifique o log do Triton para quaisquer mensagens de erro relacionadas ao carregamento do modelo.</p>"}]}
# 0. Introdução ao Triton

[Triton Inference Server](https://developer.nvidia.com/triton-inference-server) como o nome ja diz, é ==responsavel pela execução de inferências em modelos treinados== de machine learning ou deep learning de qualquer framework em qualquer processador (GPU, CPU ou outro) com o Servidor de Inferência.

<figure markdown="span">
    ![Triton Server](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNQDtfSi5QF4%2Fmaxresdefault.jpg%3Fsqp%3D-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGDIgQCh_MA8%3D%26rs%3DAOn4CLCStCtyFbCy6h_cLxxfd2c6UPdijg&f=1&nofb=1&ipt=7f96467636618dda2de08bba86adae22c2f7fd91c7f420a61d1e5fbb6faf452b&ipo=images){width=800}
</figure>

Fornece uma solução de inferência de nuvem e borda ==otimizada por CPUs e GPUs==. O Triton ==suporta um protocolo HTTP/REST e GRPC== que permite que clientes remotos solicitem inferência para qualquer modelo gerenciado pelo servidor.

## Principais recursos

- O Triton oferece ^^baixa latência e alta taxa de transfêrencia^^ para inferência de grandes modelos.
- Permite executar cargas de trabalho com IA com ^^vários modelos^^, pipelines e etapas de pré e pós-processamento.
- O ^^Analisador de Modelos^^ reduz o tempo necessário para encontrar a configuração ideal de implantação do modelo, como tamanho do lote, precisão e instâncias de execução simultânea.

## Frameworks suportados

É suportado todos os principais modelos de IA em qualquer framework importante, incluindo:

- TensorRT
- ONNX Runtime
- TensorFlow
- PyTorch
- OpenVINO
- Python
- DALI
- FIL
- TensorRT-LLM
- vLLM
- XGBoost
- LightGBM
- Scikit-Learn
- cuML

## Hardware suportado

O Servidor de Inferência Triton oferece suporte a todas as ^^GPUs NVIDIA^^, ^^CPUs x86^^, ^^Arm^^ e ^^AWS Inferentia^^.
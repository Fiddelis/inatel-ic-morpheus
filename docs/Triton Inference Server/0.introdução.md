# 0. Introdução Triton

Triton Inference Server como o nome ja diz, é ==responsavel pela execução de inferências em modelos treinados== de machine learning ou deep learning de qualquer framework em qualquer processador (GPU, CPU ou outro) com o Servidor de Inferência.

## Principais recursos

- O Triton oferece ^^baixa latência e alta taxa de transfêrencia^^ para inferência de grandes modelos.
- Permite executar cargas de trabalho com IA com ^^vários modelos^^, pipelines e etapas de pré e pós-processamento.
- O ^^Analisador de Modelos^^ reduz o tempo necessário para encontrar a configuração ideal de implantação do modelo, como tamanho do lote, precisão e instâncias de execução simultânea.

## Frameworks suportados

É suportado todos os principais modelos de IA em qualquer framework importante, incluindo:

- TensorRT
- ONNX Runtime
- TensorFlow
- PyTorch
- OpenVINO
- Python
- DALI
- FIL
- TensorRT-LLM
- vLLM
- XGBoost
- LightGBM
- Scikit-Learn
- cuML

## Hardware suportado

O Servidor de Inferência Triton oferece suporte a todas as ^^GPUs NVIDIA^^, ^^CPUs x86^^, ^^Arm^^ e ^^AWS Inferentia^^.